{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA AUGMENTATION TECHNIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting image augmentation process...\n",
      "Target folder: C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Cat_Emoji\n",
      "Current images: 55\n",
      "Generating 445 new images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:02<00:00, 164.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmentation completed!\n",
      "Final image count: 500\n",
      "Process completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_augmentation_pipeline():\n",
    "    \"\"\"Create an augmentation pipeline with various transformations\"\"\"\n",
    "    return A.Compose([\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),  # Changed from Flip to HorizontalFlip\n",
    "        A.VerticalFlip(p=0.5),    # Added VerticalFlip\n",
    "        A.Transpose(p=0.5),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(p=0.5),\n",
    "            A.MultiplicativeNoise(p=0.5),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=0.2),\n",
    "            A.MedianBlur(blur_limit=3, p=0.1),\n",
    "            A.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(p=0.3),\n",
    "            A.GridDistortion(p=0.1),\n",
    "            A.ElasticTransform(p=0.3),  # Changed from IAAPiecewiseAffine\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.CLAHE(clip_limit=2),\n",
    "            A.Sharpen(p=0.3),           # Changed from IAASharpen\n",
    "            A.Emboss(p=0.3),            # Changed from IAAEmboss\n",
    "            A.RandomBrightnessContrast(),\n",
    "        ], p=0.3),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "    ])\n",
    "\n",
    "def augment_cat_emoji(folder_path=r\"C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Cat_Emoji\", target_count=500):\n",
    "    \"\"\"\n",
    "    Augment images in the Cat_Emoji folder until reaching 500 images\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure folder exists\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Error: Folder {folder_path} does not exist\")\n",
    "            return\n",
    "\n",
    "        # Get list of existing images\n",
    "        image_files = [f for f in os.listdir(folder_path) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        if not image_files:\n",
    "            print(f\"Error: No images found in {folder_path}\")\n",
    "            return\n",
    "\n",
    "        # Get current highest number\n",
    "        current_numbers = [int(''.join(filter(str.isdigit, f))) for f in image_files]\n",
    "        start_number = max(current_numbers) + 1 if current_numbers else 1\n",
    "\n",
    "        # Calculate needed augmentations\n",
    "        num_augmentations_needed = target_count - len(image_files)\n",
    "\n",
    "        if num_augmentations_needed <= 0:\n",
    "            print(f\"Folder already has {len(image_files)} images\")\n",
    "            return\n",
    "\n",
    "        print(f\"Current images: {len(image_files)}\")\n",
    "        print(f\"Generating {num_augmentations_needed} new images...\")\n",
    "\n",
    "        # Create augmentation pipeline\n",
    "        transform = create_augmentation_pipeline()\n",
    "\n",
    "        # Generate augmented images\n",
    "        for i in tqdm(range(num_augmentations_needed)):\n",
    "            try:\n",
    "                # Randomly select an image to augment\n",
    "                source_image_name = np.random.choice(image_files)\n",
    "                source_image_path = os.path.join(folder_path, source_image_name)\n",
    "\n",
    "                # Read and process image\n",
    "                image = cv2.imread(source_image_path)\n",
    "                if image is None:\n",
    "                    print(f\"\\nError reading image: {source_image_path}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert to RGB for augmentation\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Apply augmentation\n",
    "                augmented = transform(image=image)\n",
    "                augmented_image = augmented['image']\n",
    "\n",
    "                # Convert back to BGR for saving\n",
    "                augmented_image = cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # Save augmented image\n",
    "                new_filename = f\"{start_number + i}.jpg\"\n",
    "                save_path = os.path.join(folder_path, new_filename)\n",
    "                \n",
    "                success = cv2.imwrite(save_path, augmented_image)\n",
    "                if not success:\n",
    "                    print(f\"\\nFailed to save image: {save_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing image: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        final_count = len([f for f in os.listdir(folder_path) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"\\nAugmentation completed!\")\n",
    "        print(f\"Final image count: {final_count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Specify the folder path\n",
    "        folder_path = r\"C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Cat_Emoji\"\n",
    "        \n",
    "        # Print starting message\n",
    "        print(\"Starting image augmentation process...\")\n",
    "        print(f\"Target folder: {folder_path}\")\n",
    "        \n",
    "        # Run augmentation\n",
    "        augment_cat_emoji(folder_path)\n",
    "        \n",
    "        print(\"Process completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting image augmentation process...\n",
      "Target folder: C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Hand_Emoji\n",
      "Current images: 55\n",
      "Generating 445 new images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:01<00:00, 224.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmentation completed!\n",
      "Final image count: 500\n",
      "Process completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_augmentation_pipeline():\n",
    "    \"\"\"Create an augmentation pipeline with various transformations\"\"\"\n",
    "    return A.Compose([\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),  # Changed from Flip to HorizontalFlip\n",
    "        A.VerticalFlip(p=0.5),    # Added VerticalFlip\n",
    "        A.Transpose(p=0.5),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(p=0.5),\n",
    "            A.MultiplicativeNoise(p=0.5),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=0.2),\n",
    "            A.MedianBlur(blur_limit=3, p=0.1),\n",
    "            A.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(p=0.3),\n",
    "            A.GridDistortion(p=0.1),\n",
    "            A.ElasticTransform(p=0.3),  # Changed from IAAPiecewiseAffine\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.CLAHE(clip_limit=2),\n",
    "            A.Sharpen(p=0.3),           # Changed from IAASharpen\n",
    "            A.Emboss(p=0.3),            # Changed from IAAEmboss\n",
    "            A.RandomBrightnessContrast(),\n",
    "        ], p=0.3),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "    ])\n",
    "\n",
    "def augment_Hand_emoji(folder_path=r\"C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Cat_Emoji\", target_count=500):\n",
    "    \"\"\"\n",
    "    Augment images in the Cat_Emoji folder until reaching 500 images\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure folder exists\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Error: Folder {folder_path} does not exist\")\n",
    "            return\n",
    "\n",
    "        # Get list of existing images\n",
    "        image_files = [f for f in os.listdir(folder_path) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        if not image_files:\n",
    "            print(f\"Error: No images found in {folder_path}\")\n",
    "            return\n",
    "\n",
    "        # Get current highest number\n",
    "        current_numbers = [int(''.join(filter(str.isdigit, f))) for f in image_files]\n",
    "        start_number = max(current_numbers) + 1 if current_numbers else 1\n",
    "\n",
    "        # Calculate needed augmentations\n",
    "        num_augmentations_needed = target_count - len(image_files)\n",
    "\n",
    "        if num_augmentations_needed <= 0:\n",
    "            print(f\"Folder already has {len(image_files)} images\")\n",
    "            return\n",
    "\n",
    "        print(f\"Current images: {len(image_files)}\")\n",
    "        print(f\"Generating {num_augmentations_needed} new images...\")\n",
    "\n",
    "        # Create augmentation pipeline\n",
    "        transform = create_augmentation_pipeline()\n",
    "\n",
    "        # Generate augmented images\n",
    "        for i in tqdm(range(num_augmentations_needed)):\n",
    "            try:\n",
    "                # Randomly select an image to augment\n",
    "                source_image_name = np.random.choice(image_files)\n",
    "                source_image_path = os.path.join(folder_path, source_image_name)\n",
    "\n",
    "                # Read and process image\n",
    "                image = cv2.imread(source_image_path)\n",
    "                if image is None:\n",
    "                    print(f\"\\nError reading image: {source_image_path}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert to RGB for augmentation\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Apply augmentation\n",
    "                augmented = transform(image=image)\n",
    "                augmented_image = augmented['image']\n",
    "\n",
    "                # Convert back to BGR for saving\n",
    "                augmented_image = cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # Save augmented image\n",
    "                new_filename = f\"{start_number + i}.jpg\"\n",
    "                save_path = os.path.join(folder_path, new_filename)\n",
    "                \n",
    "                success = cv2.imwrite(save_path, augmented_image)\n",
    "                if not success:\n",
    "                    print(f\"\\nFailed to save image: {save_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing image: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        final_count = len([f for f in os.listdir(folder_path) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"\\nAugmentation completed!\")\n",
    "        print(f\"Final image count: {final_count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Specify the folder path\n",
    "        folder_path = r\"C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Hand_Emoji\"\n",
    "        \n",
    "        # Print starting message\n",
    "        print(\"Starting image augmentation process...\")\n",
    "        print(f\"Target folder: {folder_path}\")\n",
    "        \n",
    "        # Run augmentation\n",
    "        augment_Hand_emoji(folder_path)\n",
    "        \n",
    "        print(\"Process completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting image augmentation process...\n",
      "Target folder: C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Happy_Face_Emoji\n",
      "Current images: 55\n",
      "Generating 445 new images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/445 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:02<00:00, 216.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmentation completed!\n",
      "Final image count: 500\n",
      "Process completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_augmentation_pipeline():\n",
    "    \"\"\"Create an augmentation pipeline with various transformations\"\"\"\n",
    "    return A.Compose([\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),  # Changed from Flip to HorizontalFlip\n",
    "        A.VerticalFlip(p=0.5),    # Added VerticalFlip\n",
    "        A.Transpose(p=0.5),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(p=0.5),\n",
    "            A.MultiplicativeNoise(p=0.5),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=0.2),\n",
    "            A.MedianBlur(blur_limit=3, p=0.1),\n",
    "            A.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(p=0.3),\n",
    "            A.GridDistortion(p=0.1),\n",
    "            A.ElasticTransform(p=0.3),  # Changed from IAAPiecewiseAffine\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.CLAHE(clip_limit=2),\n",
    "            A.Sharpen(p=0.3),           # Changed from IAASharpen\n",
    "            A.Emboss(p=0.3),            # Changed from IAAEmboss\n",
    "            A.RandomBrightnessContrast(),\n",
    "        ], p=0.3),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "    ])\n",
    "\n",
    "def augment_Happy_Face_emoji(folder_path=r\"C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Cat_Emoji\", target_count=500):\n",
    "    \"\"\"\n",
    "    Augment images in the Cat_Emoji folder until reaching 500 images\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure folder exists\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Error: Folder {folder_path} does not exist\")\n",
    "            return\n",
    "\n",
    "        # Get list of existing images\n",
    "        image_files = [f for f in os.listdir(folder_path) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        if not image_files:\n",
    "            print(f\"Error: No images found in {folder_path}\")\n",
    "            return\n",
    "\n",
    "        # Get current highest number\n",
    "        current_numbers = [int(''.join(filter(str.isdigit, f))) for f in image_files]\n",
    "        start_number = max(current_numbers) + 1 if current_numbers else 1\n",
    "\n",
    "        # Calculate needed augmentations\n",
    "        num_augmentations_needed = target_count - len(image_files)\n",
    "\n",
    "        if num_augmentations_needed <= 0:\n",
    "            print(f\"Folder already has {len(image_files)} images\")\n",
    "            return\n",
    "\n",
    "        print(f\"Current images: {len(image_files)}\")\n",
    "        print(f\"Generating {num_augmentations_needed} new images...\")\n",
    "\n",
    "        # Create augmentation pipeline\n",
    "        transform = create_augmentation_pipeline()\n",
    "\n",
    "        # Generate augmented images\n",
    "        for i in tqdm(range(num_augmentations_needed)):\n",
    "            try:\n",
    "                # Randomly select an image to augment\n",
    "                source_image_name = np.random.choice(image_files)\n",
    "                source_image_path = os.path.join(folder_path, source_image_name)\n",
    "\n",
    "                # Read and process image\n",
    "                image = cv2.imread(source_image_path)\n",
    "                if image is None:\n",
    "                    print(f\"\\nError reading image: {source_image_path}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert to RGB for augmentation\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Apply augmentation\n",
    "                augmented = transform(image=image)\n",
    "                augmented_image = augmented['image']\n",
    "\n",
    "                # Convert back to BGR for saving\n",
    "                augmented_image = cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # Save augmented image\n",
    "                new_filename = f\"{start_number + i}.jpg\"\n",
    "                save_path = os.path.join(folder_path, new_filename)\n",
    "                \n",
    "                success = cv2.imwrite(save_path, augmented_image)\n",
    "                if not success:\n",
    "                    print(f\"\\nFailed to save image: {save_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing image: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        final_count = len([f for f in os.listdir(folder_path) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"\\nAugmentation completed!\")\n",
    "        print(f\"Final image count: {final_count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Specify the folder path\n",
    "        folder_path = r\"C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Happy_Face_Emoji\"\n",
    "        \n",
    "        # Print starting message\n",
    "        print(\"Starting image augmentation process...\")\n",
    "        print(f\"Target folder: {folder_path}\")\n",
    "        \n",
    "        # Run augmentation\n",
    "        augment_Happy_Face_emoji(folder_path)\n",
    "        \n",
    "        print(\"Process completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting image augmentation process...\n",
      "Target folder: C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Heart_Emoji\n",
      "Current images: 55\n",
      "Generating 445 new images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:02<00:00, 202.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmentation completed!\n",
      "Final image count: 500\n",
      "Process completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_augmentation_pipeline():\n",
    "    \"\"\"Create an augmentation pipeline with various transformations\"\"\"\n",
    "    return A.Compose([\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),  # Changed from Flip to HorizontalFlip\n",
    "        A.VerticalFlip(p=0.5),    # Added VerticalFlip\n",
    "        A.Transpose(p=0.5),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(p=0.5),\n",
    "            A.MultiplicativeNoise(p=0.5),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=0.2),\n",
    "            A.MedianBlur(blur_limit=3, p=0.1),\n",
    "            A.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(p=0.3),\n",
    "            A.GridDistortion(p=0.1),\n",
    "            A.ElasticTransform(p=0.3),  # Changed from IAAPiecewiseAffine\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.CLAHE(clip_limit=2),\n",
    "            A.Sharpen(p=0.3),           # Changed from IAASharpen\n",
    "            A.Emboss(p=0.3),            # Changed from IAAEmboss\n",
    "            A.RandomBrightnessContrast(),\n",
    "        ], p=0.3),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "    ])\n",
    "\n",
    "def augment_Heart_emoji(folder_path=r\"C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Cat_Emoji\", target_count=500):\n",
    "    \"\"\"\n",
    "    Augment images in the Cat_Emoji folder until reaching 500 images\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure folder exists\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Error: Folder {folder_path} does not exist\")\n",
    "            return\n",
    "\n",
    "        # Get list of existing images\n",
    "        image_files = [f for f in os.listdir(folder_path) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        if not image_files:\n",
    "            print(f\"Error: No images found in {folder_path}\")\n",
    "            return\n",
    "\n",
    "        # Get current highest number\n",
    "        current_numbers = [int(''.join(filter(str.isdigit, f))) for f in image_files]\n",
    "        start_number = max(current_numbers) + 1 if current_numbers else 1\n",
    "\n",
    "        # Calculate needed augmentations\n",
    "        num_augmentations_needed = target_count - len(image_files)\n",
    "\n",
    "        if num_augmentations_needed <= 0:\n",
    "            print(f\"Folder already has {len(image_files)} images\")\n",
    "            return\n",
    "\n",
    "        print(f\"Current images: {len(image_files)}\")\n",
    "        print(f\"Generating {num_augmentations_needed} new images...\")\n",
    "\n",
    "        # Create augmentation pipeline\n",
    "        transform = create_augmentation_pipeline()\n",
    "\n",
    "        # Generate augmented images\n",
    "        for i in tqdm(range(num_augmentations_needed)):\n",
    "            try:\n",
    "                # Randomly select an image to augment\n",
    "                source_image_name = np.random.choice(image_files)\n",
    "                source_image_path = os.path.join(folder_path, source_image_name)\n",
    "\n",
    "                # Read and process image\n",
    "                image = cv2.imread(source_image_path)\n",
    "                if image is None:\n",
    "                    print(f\"\\nError reading image: {source_image_path}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert to RGB for augmentation\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Apply augmentation\n",
    "                augmented = transform(image=image)\n",
    "                augmented_image = augmented['image']\n",
    "\n",
    "                # Convert back to BGR for saving\n",
    "                augmented_image = cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # Save augmented image\n",
    "                new_filename = f\"{start_number + i}.jpg\"\n",
    "                save_path = os.path.join(folder_path, new_filename)\n",
    "                \n",
    "                success = cv2.imwrite(save_path, augmented_image)\n",
    "                if not success:\n",
    "                    print(f\"\\nFailed to save image: {save_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing image: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        final_count = len([f for f in os.listdir(folder_path) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"\\nAugmentation completed!\")\n",
    "        print(f\"Final image count: {final_count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Specify the folder path\n",
    "        folder_path = r\"C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Heart_Emoji\"\n",
    "        \n",
    "        # Print starting message\n",
    "        print(\"Starting image augmentation process...\")\n",
    "        print(f\"Target folder: {folder_path}\")\n",
    "        \n",
    "        # Run augmentation\n",
    "        augment_Heart_emoji(folder_path)\n",
    "        \n",
    "        print(\"Process completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting image augmentation process...\n",
      "Target folder: C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Sad_Emoji\n",
      "Current images: 54\n",
      "Generating 446 new images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:03<00:00, 136.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmentation completed!\n",
      "Final image count: 500\n",
      "Process completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_augmentation_pipeline():\n",
    "    \"\"\"Create an augmentation pipeline with various transformations\"\"\"\n",
    "    return A.Compose([\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),  # Changed from Flip to HorizontalFlip\n",
    "        A.VerticalFlip(p=0.5),    # Added VerticalFlip\n",
    "        A.Transpose(p=0.5),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(p=0.5),\n",
    "            A.MultiplicativeNoise(p=0.5),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=0.2),\n",
    "            A.MedianBlur(blur_limit=3, p=0.1),\n",
    "            A.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(p=0.3),\n",
    "            A.GridDistortion(p=0.1),\n",
    "            A.ElasticTransform(p=0.3),  # Changed from IAAPiecewiseAffine\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.CLAHE(clip_limit=2),\n",
    "            A.Sharpen(p=0.3),           # Changed from IAASharpen\n",
    "            A.Emboss(p=0.3),            # Changed from IAAEmboss\n",
    "            A.RandomBrightnessContrast(),\n",
    "        ], p=0.3),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "    ])\n",
    "\n",
    "def augment_Sad_emoji(folder_path=r\"C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Cat_Emoji\", target_count=500):\n",
    "    \"\"\"\n",
    "    Augment images in the Cat_Emoji folder until reaching 500 images\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure folder exists\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Error: Folder {folder_path} does not exist\")\n",
    "            return\n",
    "\n",
    "        # Get list of existing images\n",
    "        image_files = [f for f in os.listdir(folder_path) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        if not image_files:\n",
    "            print(f\"Error: No images found in {folder_path}\")\n",
    "            return\n",
    "\n",
    "        # Get current highest number\n",
    "        current_numbers = [int(''.join(filter(str.isdigit, f))) for f in image_files]\n",
    "        start_number = max(current_numbers) + 1 if current_numbers else 1\n",
    "\n",
    "        # Calculate needed augmentations\n",
    "        num_augmentations_needed = target_count - len(image_files)\n",
    "\n",
    "        if num_augmentations_needed <= 0:\n",
    "            print(f\"Folder already has {len(image_files)} images\")\n",
    "            return\n",
    "\n",
    "        print(f\"Current images: {len(image_files)}\")\n",
    "        print(f\"Generating {num_augmentations_needed} new images...\")\n",
    "\n",
    "        # Create augmentation pipeline\n",
    "        transform = create_augmentation_pipeline()\n",
    "\n",
    "        # Generate augmented images\n",
    "        for i in tqdm(range(num_augmentations_needed)):\n",
    "            try:\n",
    "                # Randomly select an image to augment\n",
    "                source_image_name = np.random.choice(image_files)\n",
    "                source_image_path = os.path.join(folder_path, source_image_name)\n",
    "\n",
    "                # Read and process image\n",
    "                image = cv2.imread(source_image_path)\n",
    "                if image is None:\n",
    "                    print(f\"\\nError reading image: {source_image_path}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert to RGB for augmentation\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Apply augmentation\n",
    "                augmented = transform(image=image)\n",
    "                augmented_image = augmented['image']\n",
    "\n",
    "                # Convert back to BGR for saving\n",
    "                augmented_image = cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # Save augmented image\n",
    "                new_filename = f\"{start_number + i}.jpg\"\n",
    "                save_path = os.path.join(folder_path, new_filename)\n",
    "                \n",
    "                success = cv2.imwrite(save_path, augmented_image)\n",
    "                if not success:\n",
    "                    print(f\"\\nFailed to save image: {save_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing image: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        final_count = len([f for f in os.listdir(folder_path) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"\\nAugmentation completed!\")\n",
    "        print(f\"Final image count: {final_count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Specify the folder path\n",
    "        folder_path = r\"C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Sad_Emoji\"\n",
    "        \n",
    "        # Print starting message\n",
    "        print(\"Starting image augmentation process...\")\n",
    "        print(f\"Target folder: {folder_path}\")\n",
    "        \n",
    "        # Run augmentation\n",
    "        augment_Sad_emoji(folder_path)\n",
    "        \n",
    "        print(\"Process completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test set extraction process...\n",
      "Source folder: C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\train\\Cat_Emoji\n",
      "Test folder: C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\train\\Cat_Emoji_test\n",
      "\n",
      "Copying 100 images to test folder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 221.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully copied 100 images to C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\train\\Cat_Emoji_test\n",
      "\n",
      "Process completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_test_images(source_folder, num_images=100):\n",
    "    \"\"\"\n",
    "    Extract random images from source folder to create test set\n",
    "    Args:\n",
    "        source_folder: Path to Cat_Emoji folder\n",
    "        num_images: Number of images to extract for test set\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the parent directory of source folder\n",
    "        parent_dir = os.path.dirname(source_folder)\n",
    "        \n",
    "        # Create test directory\n",
    "        test_folder = os.path.join(parent_dir, \"Cat_Emoji_test\")\n",
    "        os.makedirs(test_folder, exist_ok=True)\n",
    "        \n",
    "        print(f\"Source folder: {source_folder}\")\n",
    "        print(f\"Test folder: {test_folder}\")\n",
    "        \n",
    "        # Get all images from source folder\n",
    "        images = [f for f in os.listdir(source_folder) \n",
    "                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        if len(images) < num_images:\n",
    "            print(f\"Warning: Only {len(images)} images found in source folder\")\n",
    "            num_images = len(images)\n",
    "        \n",
    "        # Randomly select images\n",
    "        selected_images = random.sample(images, num_images)\n",
    "        \n",
    "        print(f\"\\nCopying {num_images} images to test folder...\")\n",
    "        \n",
    "        # Copy selected images to test folder\n",
    "        for img in tqdm(selected_images):\n",
    "            src = os.path.join(source_folder, img)\n",
    "            dst = os.path.join(test_folder, img)\n",
    "            shutil.copy2(src, dst)\n",
    "        \n",
    "        print(f\"\\nSuccessfully copied {num_images} images to {test_folder}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # Source folder path\n",
    "    source_folder = r\"C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\Cat_Emoji\"\n",
    "    \n",
    "    print(\"Starting test set extraction process...\")\n",
    "    \n",
    "    # Extract test images\n",
    "    extract_test_images(source_folder)\n",
    "    \n",
    "    print(\"\\nProcess completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (2500, 4096), Training Labels Shape: (2500, 5)\n",
      "Testing Data Shape: (578, 4096), Testing Labels Shape: (578, 5)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Classes\n",
    "classes = ['Cat_Emoji', 'Hand_Emoji', 'Happy_Face_Emoji', 'Heart_Emoji', 'Sad_Emoji']\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_images(data_path, img_size=64):\n",
    "    \"\"\"\n",
    "    Preprocess images: Load them in grayscale, resize, normalize, and flatten.\n",
    "    Args:\n",
    "        data_path: Path to the dataset folder containing class subfolders.\n",
    "        img_size: Desired size for the resized image (img_size x img_size).\n",
    "\n",
    "    Returns:\n",
    "        data: Array of flattened images.\n",
    "        labels: Array of corresponding labels.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for label, category in enumerate(classes):\n",
    "        category_path = os.path.join(data_path, category)\n",
    "        if not os.path.exists(category_path):\n",
    "            print(f\"Warning: {category_path} does not exist!\")\n",
    "            continue\n",
    "        \n",
    "        for img_name in os.listdir(category_path):\n",
    "            img_path = os.path.join(category_path, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            img_resized = cv2.resize(img, (img_size, img_size))  # Resize to 64x64\n",
    "            data.append(img_resized.flatten() / 255.0)  # Normalize and flatten\n",
    "            labels.append(label)  # Assign label (0-4)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# One-hot encoding for labels\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    \"\"\"\n",
    "    Converts integer labels to one-hot encoded vectors.\n",
    "    Args:\n",
    "        labels: Array of integer labels.\n",
    "        num_classes: Number of classes for one-hot encoding.\n",
    "\n",
    "    Returns:\n",
    "        One-hot encoded label array.\n",
    "    \"\"\"\n",
    "    encoded = np.zeros((labels.size, num_classes))\n",
    "    encoded[np.arange(labels.size), labels] = 1\n",
    "    return encoded\n",
    "\n",
    "# Paths to train and test folders\n",
    "train_path = r\"C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\train\"\n",
    "test_path = r\"C:\\Users\\Haseeb Raza\\Desktop\\Bscs22115_AI_dataset\\test\"\n",
    "\n",
    "# Preprocess training and testing data\n",
    "X_train, y_train = preprocess_images(train_path, img_size=64)\n",
    "X_test, y_test = preprocess_images(test_path, img_size=64)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = one_hot_encode(y_train, num_classes=num_classes)\n",
    "y_test = one_hot_encode(y_test, num_classes=num_classes)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Training Data Shape: {X_train.shape}, Training Labels Shape: {y_train.shape}\")\n",
    "print(f\"Testing Data Shape: {X_test.shape}, Testing Labels Shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Initialize ANN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "input_size = 64 * 64  # Flattened image size\n",
    "hidden_size = 128  # Number of neurons in the hidden layer\n",
    "output_size = 5  # Number of classes (Cat_Emoji, Hand_Emoji, etc.)\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Randomly initialize weights and biases\n",
    "W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "b2 = np.zeros((output_size, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Define Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z))  # Numerical stability\n",
    "    return expZ / expZ.sum(axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X):\n",
    "    global W1, b1, W2, b2\n",
    "    Z1 = np.dot(W1, X.T) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    cache = (Z1, A1, Z2, A2)\n",
    "    return A2, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y, A2):\n",
    "    m = Y.shape[0]\n",
    "    log_probs = -np.log(A2.T[range(m), np.argmax(Y, axis=1)])\n",
    "    loss = np.sum(log_probs) / m\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, cache):\n",
    "    global W1, b1, W2, b2\n",
    "    Z1, A1, Z2, A2 = cache\n",
    "    m = X.shape[0]\n",
    "\n",
    "    dZ2 = A2.T - Y\n",
    "    dW2 = np.dot(dZ2.T, A1.T) / m\n",
    "    db2 = np.sum(dZ2.T, axis=1, keepdims=True) / m\n",
    "    dZ1 = np.dot(W2.T, dZ2.T) * relu_derivative(Z1)\n",
    "    dW1 = np.dot(dZ1, X) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "    return dW1, db1, dW2, db2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(dW1, db1, dW2, db2):\n",
    "    global W1, b1, W2, b2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Train the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 1.6072365803106032\n",
      "Epoch 100: Loss = 1.5495078211954292\n",
      "Epoch 200: Loss = 1.4093092370873659\n",
      "Epoch 300: Loss = 1.2447975772944924\n",
      "Epoch 400: Loss = 1.1067590955771487\n",
      "Epoch 500: Loss = 0.9924727644464885\n",
      "Epoch 600: Loss = 0.8974814901913116\n",
      "Epoch 700: Loss = 0.8163475523199666\n",
      "Epoch 800: Loss = 0.7444925886509395\n",
      "Epoch 900: Loss = 0.6799241094691262\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    A2, cache = forward_propagation(X_train)\n",
    "    loss = compute_loss(y_train, A2)\n",
    "    dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, cache)\n",
    "    update_parameters(dW1, db1, dW2, db2)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.08%\n"
     ]
    }
   ],
   "source": [
    "def predict(X):\n",
    "    A2, _ = forward_propagation(X)\n",
    "    return np.argmax(A2, axis=0)\n",
    "\n",
    "y_pred = predict(X_test)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_true) * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Save Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the parameters to files\n",
    "np.save('W1.npy', W1)\n",
    "np.save('b1.npy', b1)\n",
    "np.save('W2.npy', W2)\n",
    "np.save('b2.npy', b2)\n",
    "\n",
    "print(\"Model parameters saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10: Load Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved parameters\n",
    "W1 = np.load('W1.npy')\n",
    "b1 = np.load('b1.npy')\n",
    "W2 = np.load('W2.npy')\n",
    "b2 = np.load('b2.npy')\n",
    "\n",
    "print(\"Model parameters loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11: Classify a Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_single_image(img_path, img_size=64):\n",
    "    \"\"\"Preprocess a single image for prediction.\"\"\"\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n",
    "    img_resized = cv2.resize(img, (img_size, img_size))  # Resize to 64x64\n",
    "    img_flattened = img_resized.flatten() / 255.0  # Flatten and normalize\n",
    "    return img_flattened\n",
    "\n",
    "def classify_image(img_path):\n",
    "    \"\"\"Classify a single image.\"\"\"\n",
    "    X = preprocess_single_image(img_path)\n",
    "    X = X.reshape(1, -1)  # Ensure input shape matches the model's expectation\n",
    "    A2, _ = forward_propagation(X)\n",
    "    predicted_label = np.argmax(A2, axis=0)[0]\n",
    "    \n",
    "    # Map numerical label to category\n",
    "    categories = ['Cat_Emoji', 'Hand_Emoji', 'Happy_Face_Emoji', 'Heart_Emoji', 'Sad_Emoji']\n",
    "    return categories[predicted_label]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 12: Test Classifier on New Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class for the image is: Happy_Face_Emoji\n"
     ]
    }
   ],
   "source": [
    "new_image_path = r'test\\Hand_Emoji\\23.png'  # Update with your image path\n",
    "predicted_class = classify_image(new_image_path)\n",
    "print(f\"The predicted class for the image is: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 1.6111\n",
      "Epoch 10: Loss = 1.6045\n",
      "Epoch 20: Loss = 1.5985\n",
      "Epoch 30: Loss = 1.5932\n",
      "Epoch 40: Loss = 1.5881\n",
      "Epoch 50: Loss = 1.5827\n",
      "Epoch 60: Loss = 1.5766\n",
      "Epoch 70: Loss = 1.5697\n",
      "Epoch 80: Loss = 1.5617\n",
      "Epoch 90: Loss = 1.5525\n",
      "Epoch 100: Loss = 1.5422\n",
      "Epoch 110: Loss = 1.5306\n",
      "Epoch 120: Loss = 1.5179\n",
      "Epoch 130: Loss = 1.5041\n",
      "Epoch 140: Loss = 1.4893\n",
      "Epoch 150: Loss = 1.4739\n",
      "Epoch 160: Loss = 1.4580\n",
      "Epoch 170: Loss = 1.4417\n",
      "Epoch 180: Loss = 1.4251\n",
      "Epoch 190: Loss = 1.4083\n",
      "Epoch 200: Loss = 1.3912\n",
      "Epoch 210: Loss = 1.3740\n",
      "Epoch 220: Loss = 1.3567\n",
      "Epoch 230: Loss = 1.3393\n",
      "Epoch 240: Loss = 1.3222\n",
      "Epoch 250: Loss = 1.3052\n",
      "Epoch 260: Loss = 1.2885\n",
      "Epoch 270: Loss = 1.2721\n",
      "Epoch 280: Loss = 1.2560\n",
      "Epoch 290: Loss = 1.2403\n",
      "Epoch 300: Loss = 1.2250\n",
      "Epoch 310: Loss = 1.2101\n",
      "Epoch 320: Loss = 1.1955\n",
      "Epoch 330: Loss = 1.1812\n",
      "Epoch 340: Loss = 1.1673\n",
      "Epoch 350: Loss = 1.1537\n",
      "Epoch 360: Loss = 1.1403\n",
      "Epoch 370: Loss = 1.1272\n",
      "Epoch 380: Loss = 1.1144\n",
      "Epoch 390: Loss = 1.1018\n",
      "Epoch 400: Loss = 1.0894\n",
      "Epoch 410: Loss = 1.0772\n",
      "Epoch 420: Loss = 1.0653\n",
      "Epoch 430: Loss = 1.0536\n",
      "Epoch 440: Loss = 1.0414\n",
      "Epoch 450: Loss = 1.0298\n",
      "Epoch 460: Loss = 1.0185\n",
      "Epoch 470: Loss = 1.0074\n",
      "Epoch 480: Loss = 0.9966\n",
      "Epoch 490: Loss = 0.9859\n",
      "Epoch 500: Loss = 0.9755\n",
      "Epoch 510: Loss = 0.9652\n",
      "Epoch 520: Loss = 0.9552\n",
      "Epoch 530: Loss = 0.9453\n",
      "Epoch 540: Loss = 0.9356\n",
      "Epoch 550: Loss = 0.9260\n",
      "Epoch 560: Loss = 0.9166\n",
      "Epoch 570: Loss = 0.9073\n",
      "Epoch 580: Loss = 0.8982\n",
      "Epoch 590: Loss = 0.8893\n",
      "Epoch 600: Loss = 0.8804\n",
      "Epoch 610: Loss = 0.8717\n",
      "Epoch 620: Loss = 0.8631\n",
      "Epoch 630: Loss = 0.8546\n",
      "Epoch 640: Loss = 0.8463\n",
      "Epoch 650: Loss = 0.8380\n",
      "Epoch 660: Loss = 0.8299\n",
      "Epoch 670: Loss = 0.8218\n",
      "Epoch 680: Loss = 0.8139\n",
      "Epoch 690: Loss = 0.8060\n",
      "Epoch 700: Loss = 0.7982\n",
      "Epoch 710: Loss = 0.7906\n",
      "Epoch 720: Loss = 0.7830\n",
      "Epoch 730: Loss = 0.7755\n",
      "Epoch 740: Loss = 0.7680\n",
      "Epoch 750: Loss = 0.7607\n",
      "Epoch 760: Loss = 0.7534\n",
      "Epoch 770: Loss = 0.7463\n",
      "Epoch 780: Loss = 0.7392\n",
      "Epoch 790: Loss = 0.7322\n",
      "Epoch 800: Loss = 0.7253\n",
      "Epoch 810: Loss = 0.7184\n",
      "Epoch 820: Loss = 0.7116\n",
      "Epoch 830: Loss = 0.7050\n",
      "Epoch 840: Loss = 0.6984\n",
      "Epoch 850: Loss = 0.6918\n",
      "Epoch 860: Loss = 0.6854\n",
      "Epoch 870: Loss = 0.6790\n",
      "Epoch 880: Loss = 0.6727\n",
      "Epoch 890: Loss = 0.6665\n",
      "Epoch 900: Loss = 0.6604\n",
      "Epoch 910: Loss = 0.6544\n",
      "Epoch 920: Loss = 0.6484\n",
      "Epoch 930: Loss = 0.6425\n",
      "Epoch 940: Loss = 0.6367\n",
      "Epoch 950: Loss = 0.6309\n",
      "Epoch 960: Loss = 0.6252\n",
      "Epoch 970: Loss = 0.6196\n",
      "Epoch 980: Loss = 0.6141\n",
      "Epoch 990: Loss = 0.6086\n",
      "Test Accuracy: 84.95%\n",
      "Model parameters saved.\n",
      "The predicted class for the image is: Heart_Emoji\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 64 * 64  # Flattened image size (64x64 grayscale images)\n",
    "hidden_size = 128  # Number of neurons in the hidden layer\n",
    "output_size = 5  # Number of classes (5 emojis)\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Randomly initialize weights and biases\n",
    "W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Activation functions\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0))  # Numerical stability\n",
    "    return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X):\n",
    "    global W1, b1, W2, b2\n",
    "    Z1 = np.dot(W1, X.T) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    cache = (Z1, A1, Z2, A2)\n",
    "    return A2, cache\n",
    "\n",
    "# Compute loss (categorical cross-entropy)\n",
    "def compute_loss(Y, A2):\n",
    "    m = Y.shape[0]\n",
    "    log_probs = -np.log(A2.T[range(m), np.argmax(Y, axis=1)])\n",
    "    loss = np.sum(log_probs) / m\n",
    "    return loss\n",
    "\n",
    "# Backward propagation\n",
    "def backward_propagation(X, Y, cache):\n",
    "    global W1, b1, W2, b2\n",
    "    Z1, A1, Z2, A2 = cache\n",
    "    m = X.shape[0]\n",
    "\n",
    "    dZ2 = A2.T - Y\n",
    "    dW2 = np.dot(dZ2.T, A1.T) / m\n",
    "    db2 = np.sum(dZ2.T, axis=1, keepdims=True) / m\n",
    "    dZ1 = np.dot(W2.T, dZ2.T) * relu_derivative(Z1)\n",
    "    dW1 = np.dot(dZ1, X) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# Update parameters\n",
    "def update_parameters(dW1, db1, dW2, db2):\n",
    "    global W1, b1, W2, b2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "# Prediction function\n",
    "def predict(X):\n",
    "    A2, _ = forward_propagation(X)\n",
    "    return np.argmax(A2, axis=0)\n",
    "\n",
    "# Preprocess functions\n",
    "def preprocess_images(path, img_size=64):\n",
    "    images = []\n",
    "    labels = []\n",
    "    categories = ['Cat_Emoji', 'Hand_Emoji', 'Happy_Face_Emoji', 'Heart_Emoji', 'Sad_Emoji']\n",
    "\n",
    "    for label, category in enumerate(categories):\n",
    "        category_path = f\"{path}/{category}\"\n",
    "        for img_name in os.listdir(category_path):\n",
    "            img_path = f\"{category_path}/{img_name}\"\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img_resized = cv2.resize(img, (img_size, img_size))\n",
    "            images.append(img_resized.flatten() / 255.0)\n",
    "            labels.append(label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    m = len(labels)\n",
    "    encoded = np.zeros((m, num_classes))\n",
    "    for i, label in enumerate(labels):\n",
    "        encoded[i, label] = 1\n",
    "    return encoded\n",
    "\n",
    "# Preprocess data\n",
    "train_path = \"train\"\n",
    "test_path = \"test\"\n",
    "num_classes = 5\n",
    "X_train, y_train = preprocess_images(train_path)\n",
    "X_test, y_test = preprocess_images(test_path)\n",
    "\n",
    "y_train = one_hot_encode(y_train, num_classes)\n",
    "y_test = one_hot_encode(y_test, num_classes)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    A2, cache = forward_propagation(X_train)\n",
    "    loss = compute_loss(y_train, A2)\n",
    "    dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, cache)\n",
    "    update_parameters(dW1, db1, dW2, db2)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = predict(X_test)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(y_pred == y_true) * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Save model parameters\n",
    "np.save('W1.npy', W1)\n",
    "np.save('b1.npy', b1)\n",
    "np.save('W2.npy', W2)\n",
    "np.save('b2.npy', b2)\n",
    "print(\"Model parameters saved.\")\n",
    "\n",
    "# Classification of a new image\n",
    "def preprocess_single_image(img_path, img_size=64):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n",
    "    img_resized = cv2.resize(img, (img_size, img_size))  # Resize\n",
    "    img_flattened = img_resized.flatten() / 255.0  # Flatten and normalize\n",
    "    return img_flattened\n",
    "\n",
    "def classify_image(img_path):\n",
    "    global W1, b1, W2, b2\n",
    "    X = preprocess_single_image(img_path)\n",
    "    X = X.reshape(1, -1)  # Reshape for input\n",
    "    A2, _ = forward_propagation(X)\n",
    "    predicted_label = np.argmax(A2, axis=0)[0]\n",
    "    categories = ['Cat_Emoji', 'Hand_Emoji', 'Happy_Face_Emoji', 'Heart_Emoji', 'Sad_Emoji']\n",
    "    return categories[predicted_label]\n",
    "\n",
    "# Test classification on a new image\n",
    "new_image_path = r'test/Hand_Emoji/22.png'  # Replace with your test image path\n",
    "predicted_class = classify_image(new_image_path)\n",
    "print(f\"The predicted class for the image is: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 1.6111\n",
      "Epoch 100: Loss = 1.5422\n",
      "Epoch 200: Loss = 1.3912\n",
      "Epoch 300: Loss = 1.2250\n",
      "Epoch 400: Loss = 1.0894\n",
      "Epoch 500: Loss = 0.9755\n",
      "Epoch 600: Loss = 0.8804\n",
      "Epoch 700: Loss = 0.7982\n",
      "Epoch 800: Loss = 0.7253\n",
      "Epoch 900: Loss = 0.6604\n",
      "Epoch 1000: Loss = 0.6033\n",
      "Epoch 1100: Loss = 0.5530\n",
      "Epoch 1200: Loss = 0.5088\n",
      "Epoch 1300: Loss = 0.4698\n",
      "Epoch 1400: Loss = 0.4354\n",
      "Epoch 1500: Loss = 0.4053\n",
      "Epoch 1600: Loss = 0.4188\n",
      "Epoch 1700: Loss = 0.3748\n",
      "Epoch 1800: Loss = 0.3532\n",
      "Epoch 1900: Loss = 0.3330\n",
      "Epoch 2000: Loss = 0.3150\n",
      "Epoch 2100: Loss = 0.2967\n",
      "Epoch 2200: Loss = 0.2828\n",
      "Epoch 2300: Loss = 0.2674\n",
      "Epoch 2400: Loss = 0.2563\n",
      "Epoch 2500: Loss = 0.2424\n",
      "Epoch 2600: Loss = 0.2346\n",
      "Epoch 2700: Loss = 0.2218\n",
      "Epoch 2800: Loss = 0.2089\n",
      "Epoch 2900: Loss = 0.2042\n",
      "Epoch 3000: Loss = 0.2209\n",
      "Epoch 3100: Loss = 0.1846\n",
      "Epoch 3200: Loss = 0.1779\n",
      "Epoch 3300: Loss = 0.1717\n",
      "Epoch 3400: Loss = 0.1659\n",
      "Epoch 3500: Loss = 0.1604\n",
      "Epoch 3600: Loss = 0.1553\n",
      "Epoch 3700: Loss = 0.1506\n",
      "Epoch 3800: Loss = 0.1456\n",
      "Epoch 3900: Loss = 0.1404\n",
      "Epoch 4000: Loss = 0.1359\n",
      "Epoch 4100: Loss = 0.1319\n",
      "Epoch 4200: Loss = 0.1280\n",
      "Epoch 4300: Loss = 0.1240\n",
      "Epoch 4400: Loss = 0.1204\n",
      "Epoch 4500: Loss = 0.1169\n",
      "Epoch 4600: Loss = 0.1136\n",
      "Epoch 4700: Loss = 0.1104\n",
      "Epoch 4800: Loss = 0.1074\n",
      "Epoch 4900: Loss = 0.1044\n",
      "Test Accuracy: 97.92%\n",
      "Model parameters saved.\n",
      "The predicted class for the image is: Hand_Emoji\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 64 * 64  # Flattened image size (64x64 grayscale images)\n",
    "hidden_size = 128  # Number of neurons in the hidden layer\n",
    "output_size = 5  # Number of classes (5 emojis)\n",
    "learning_rate = 0.01\n",
    "epochs = 5000\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Activation functions\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0))  # Numerical stability\n",
    "    return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X):\n",
    "    Z1 = np.dot(W1, X.T) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    cache = (Z1, A1, Z2, A2)\n",
    "    return A2, cache\n",
    "\n",
    "# Compute loss (categorical cross-entropy)\n",
    "def compute_loss(Y, A2):\n",
    "    m = Y.shape[0]\n",
    "    log_probs = -np.log(A2.T[range(m), np.argmax(Y, axis=1)])\n",
    "    loss = np.sum(log_probs) / m\n",
    "    return loss\n",
    "\n",
    "# Backward propagation\n",
    "def backward_propagation(X, Y, cache):\n",
    "    Z1, A1, Z2, A2 = cache\n",
    "    m = X.shape[0]\n",
    "\n",
    "    dZ2 = A2.T - Y\n",
    "    dW2 = np.dot(dZ2.T, A1.T) / m\n",
    "    db2 = np.sum(dZ2.T, axis=1, keepdims=True) / m\n",
    "    dZ1 = np.dot(W2.T, dZ2.T) * relu_derivative(Z1)\n",
    "    dW1 = np.dot(dZ1, X) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# Update parameters\n",
    "def update_parameters(dW1, db1, dW2, db2):\n",
    "    global W1, b1, W2, b2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "# Prediction function\n",
    "def predict(X):\n",
    "    A2, _ = forward_propagation(X)\n",
    "    return np.argmax(A2, axis=0)\n",
    "\n",
    "# Preprocess functions\n",
    "def preprocess_images(path, img_size=64):\n",
    "    images = []\n",
    "    labels = []\n",
    "    categories = ['Cat_Emoji', 'Hand_Emoji', 'Happy_Face_Emoji', 'Heart_Emoji', 'Sad_Emoji']\n",
    "\n",
    "    for label, category in enumerate(categories):\n",
    "        category_path = f\"{path}/{category}\"\n",
    "        for img_name in os.listdir(category_path):\n",
    "            img_path = f\"{category_path}/{img_name}\"\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img_resized = cv2.resize(img, (img_size, img_size))\n",
    "            images.append(img_resized.flatten() / 255.0)\n",
    "            labels.append(label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    m = len(labels)\n",
    "    encoded = np.zeros((m, num_classes))\n",
    "    for i, label in enumerate(labels):\n",
    "        encoded[i, label] = 1\n",
    "    return encoded\n",
    "\n",
    "# Preprocess data\n",
    "train_path = \"train\"\n",
    "test_path = \"test\"\n",
    "num_classes = 5\n",
    "X_train, y_train = preprocess_images(train_path)\n",
    "X_test, y_test = preprocess_images(test_path)\n",
    "\n",
    "y_train = one_hot_encode(y_train, num_classes)\n",
    "y_test = one_hot_encode(y_test, num_classes)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    A2, cache = forward_propagation(X_train)\n",
    "    loss = compute_loss(y_train, A2)\n",
    "    dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, cache)\n",
    "    update_parameters(dW1, db1, dW2, db2)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = predict(X_test)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(y_pred == y_true) * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Save model parameters\n",
    "np.save('W1.npy', W1)\n",
    "np.save('b1.npy', b1)\n",
    "np.save('W2.npy', W2)\n",
    "np.save('b2.npy', b2)\n",
    "print(\"Model parameters saved.\")\n",
    "\n",
    "# Classification of a new image\n",
    "def preprocess_single_image(img_path, img_size=64):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n",
    "    img_resized = cv2.resize(img, (img_size, img_size))  # Resize\n",
    "    img_flattened = img_resized.flatten() / 255.0  # Flatten and normalize\n",
    "    return img_flattened\n",
    "\n",
    "def classify_image(img_path):\n",
    "    global W1, b1, W2, b2\n",
    "    X = preprocess_single_image(img_path)\n",
    "    X = X.reshape(1, -1)  # Reshape for input\n",
    "    A2, _ = forward_propagation(X)\n",
    "    predicted_label = np.argmax(A2, axis=0)[0]\n",
    "    categories = ['Cat_Emoji', 'Hand_Emoji', 'Happy_Face_Emoji', 'Heart_Emoji', 'Sad_Emoji']\n",
    "    return categories[predicted_label]\n",
    "\n",
    "# Test classification on a new image\n",
    "new_image_path = r'test/Hand_Emoji/22.png'  # Replace with your test image path\n",
    "predicted_class = classify_image(new_image_path)\n",
    "print(f\"The predicted class for the image is: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class for the image is: Cat_Emoji\n"
     ]
    }
   ],
   "source": [
    "new_image_path = r'test/Cat_Emoji/12.png'  # Replace with your test image path\n",
    "predicted_class = classify_image(new_image_path)\n",
    "print(f\"The predicted class for the image is: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
